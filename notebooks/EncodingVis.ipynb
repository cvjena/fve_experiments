{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher Vector Encoding Basics\n",
    "\n",
    "GMM with $K$ components and diagonal covariance matrix:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    p(\\vec{x}_n|\\Theta) = \\sum_{k=1}^{K} \\alpha_k p_k(\\vec{x}_n|\\theta_k) \\\\\n",
    "\tp_k(\\vec{x}_n|\\vec{\\mu}_k, \\vec{\\sigma}_k)\n",
    "\t= \\dfrac{1}{\\prod_{d=1}^{D}\\sqrt{2\\pi} \\sigma_{k,d}}\n",
    "    \\exp\\left(-\\sum_{d=1}^{D}\\frac{(x_{n,d} - \\mu_{k,d})^2}{2 (\\sigma_{k,d})^2}\\right)\\quad.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "FVE based on this GMM:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\t\\mathcal{F}_{\\mu_{k,d}}(\\mathcal{X}_I)\n",
    "\t\t&= \\dfrac{1}{\\sqrt{N_I\\alpha_k}}\\sum_{n=1}^{N_I} w_{n,k} \\left(\\dfrac{x_{n,d} - \\mu_{k,d}}{\\sigma_{k,d}}\\right)\\quad,\n",
    "\t\t\\\\\n",
    "\t\\mathcal{F}_{\\sigma_{k,d}}(\\mathcal{X}_I)\n",
    "\t\t&= \\dfrac{1}{\\sqrt{2N_I\\alpha_k}}\\sum_{n=1}^{N_I} w_{n,k} \\left(\\dfrac{(x_{n,d} - \\mu_{k,d})^2}{(\\sigma_{k,d})^2} - 1\\right)\\quad,\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "with soft assignment\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\tw_{n,k} = \\dfrac{\\alpha_k p_k(\\vec{x}_n|\\theta_k)}{\\sum_{\\ell=1}^{K}\\alpha_\\ell p_\\ell(\\vec{x}_n|\\theta_\\ell)}\\quad.\n",
    "\t\\label{eq:soft_assignment}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The gradients of this encoding w.r.t. an input feature $x_{n,d_*}$ are:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\t\\frac{\\partial\\mathcal{F}_{\\mu_{k,d}}(\\mathcal{X}_I)}{\\partial{x_{n,d_*}}}\n",
    "\t\t&= \\dfrac{1}{\\sqrt{N_I\\alpha_k}}\\left[\\frac{\\partial{w_{n,k}}}{\\partial{x_{n,d_*}}}\\left(\\dfrac{x_{n,d} - \\mu_{k,d}}{\\sigma_{k,d}}\\right) + \\delta_{d,d_*}\\frac{w_{n,k}}{\\sigma_{k,d_*}}\\right] \\quad,\\\\\n",
    "\t\\frac{\\partial\\mathcal{F}_{\\sigma_{k,d}}(\\mathcal{X}_I)}{\\partial{x_{n,d_*}}}\n",
    "\t\t&= \\dfrac{1}{\\sqrt{2N_I\\alpha_k}}\\left[\n",
    "\t\t     \\frac{\\partial{w_{n,k}}}{\\partial{x_{n,d_*}}}\\left(\\dfrac{(x_{n,d} - \\mu_{k,d})^2}{(\\sigma_{k,d})^2} - 1\\right)\\right. \\nonumber \\\\\n",
    "\t\t     & \\phantom{==\\dfrac{1}{\\sqrt{2N_I\\alpha_k}}}\\left.+\\delta_{d,d_*}\\frac{2 w_{n,k}(x_{n,d_*}-\\mu_{k,d_*})}{(\\sigma_{k,d_*})^2}\\right]\\quad.\n",
    "\\end{align}\n",
    "$$\n",
    "In both equations, we use $\\delta_{d,d_*}$ to denote the Kronecker delta being $1$ if $d=d_*$ and $0$ else, as well as the derivative of $w_{n,k}$ w.r.t. $x_{n,d_*}$ that is given by:\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial{w_{n,k}}}{\\partial{x_{n,d_*}}} = w_{n,k}\\left(-\\frac{(x_{n,d_*}-\\mu_{k,d_*})}{(\\sigma_{k,d_*})^2} + \\sum_{\\ell=1}^K w_{n,\\ell}\\frac{(x_{n,d_*}-\\mu_{\\ell,d_*})}{(\\sigma_{\\ell,d_*})^2} \\right)\\quad.\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chainer version: 7.7.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "import cv2\n",
    "import abc\n",
    "import sys\n",
    "\n",
    "from chainer import functions as F\n",
    "\n",
    "from ipywidgets import interactive\n",
    "from tabulate import tabulate\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "from fve_layer.backends.chainer import links\n",
    "\n",
    "print(f\"Chainer version: {chainer.__version__}\")\n",
    "\n",
    "chainer.config.train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    n_components:       int     = 3\n",
    "    n_dims:             int     = 2\n",
    "        \n",
    "    init_mu:            float   = 5 # ~ U(-init_mu, init_mu)\n",
    "    rnd_mu:             bool    = True\n",
    "    init_sig:           float   = 1\n",
    "    eps:                float   = 1e-2\n",
    "        \n",
    "    x_range:            Tuple[float, float] = (-10, 10)\n",
    "    y_range:            Tuple[float, float] = (-10, 10)\n",
    "    n_samples:          int     = 25\n",
    "        \n",
    "    \n",
    "    seed:               int     = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Layer initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_layer(args: Args, mu=None, sig=None, N=None):\n",
    "    \n",
    "    init_mu = args.init_mu if mu is None else mu\n",
    "    n_comp = args.n_components if N is None else N\n",
    "    \n",
    "    # the random mu initialization (uniform in [-init_mu, init_mu]) is realized by the layer\n",
    "    if not args.rnd_mu:\n",
    "        init_mu = np.full((args.n_dims, n_comp), init_mu, dtype=np.float32)\n",
    "        \n",
    "\n",
    "    layer = links.FVELayer_noEM(\n",
    "        in_size=args.n_dims,\n",
    "        n_components=n_comp,\n",
    "\n",
    "        init_mu=init_mu,\n",
    "        init_sig=args.init_sig if sig is None else sig,\n",
    "\n",
    "        eps=args.eps\n",
    "    )\n",
    "    layer.cleargrads()\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data_grid(args: Args):\n",
    "    \n",
    "    xs,ys = [np.linspace(*r, num=args.n_samples) for r in [args.x_range, args.y_range]]\n",
    "    grid = np.stack(np.meshgrid(ys, xs), axis=2).astype(np.float32)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_classes, n_samples, *, rnd=None, overlap=False, map_size=1, dtype=np.float32):\n",
    "    Xs, ys = [], []\n",
    "    \n",
    "    if rnd is None:\n",
    "        rnd = np.random.RandomState()\n",
    "\n",
    "    _comp_pos = np.linspace(0, 2*np.pi, n_classes, endpoint=False)\n",
    "    _means = np.vstack([np.cos(_comp_pos), np.sin(_comp_pos)]).T\n",
    "    _std_factor = 2 if overlap else 1\n",
    "    _std = np.array([1., 1.]) / n_classes * _std_factor\n",
    "\n",
    "    for i, mean in enumerate(_means):\n",
    "        X = rnd.normal(loc=mean, scale=_std, size=(n_samples, 2))\n",
    "        X = X.astype(dtype)\n",
    "\n",
    "        X = np.expand_dims(np.expand_dims(X, 1), 1)\n",
    "        _pad_size = (map_size - 1) // 2\n",
    "        pad = (_pad_size, _pad_size)\n",
    "        X = np.pad(X, pad_width=[(0,0), pad, pad, (0,0)], mode=\"constant\")\n",
    "\n",
    "        noise = rnd.normal(loc=(0, 0), scale=(0.01, 0.01), size=X.shape)\n",
    "        noise_mask = X==0\n",
    "        X[noise_mask] = noise[noise_mask]\n",
    "\n",
    "        X = X.reshape(n_samples, -1, 2)\n",
    "\n",
    "        y = np.full(shape=n_samples, fill_value=i, dtype=dtype)\n",
    "        y = np.expand_dims(y, 1)\n",
    "        y = np.broadcast_to(y, (n_samples, X.shape[1]))\n",
    "\n",
    "        Xs.append(X)\n",
    "        ys.append(y)\n",
    "\n",
    "    Xs = np.vstack(Xs)\n",
    "    ys = np.vstack(ys)\n",
    "\n",
    "    return Xs*10, ys, _means*10, _std*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, *_ = generate_data(4, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 1, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm(arr, axis=(0,1)):\n",
    "    _min = arr.min(axis=axis)\n",
    "    arr[:] -= _min\n",
    "    _max = arr.max(axis=axis, keepdims=True)\n",
    "    _max_mask = _max != 0\n",
    "    if _max_mask.any():\n",
    "        arr_mask = np.broadcast_to(_max_mask, arr.shape)\n",
    "        arr[arr_mask] /= np.broadcast_to(_max, arr.shape)[arr_mask]\n",
    "    \n",
    "    return _min, np.squeeze(_max)\n",
    "\n",
    "def _denorm(arr, _min, _max):\n",
    "    _max_mask = _max != 0\n",
    "    if _max_mask.any():\n",
    "        arr = arr.copy()\n",
    "        arr[_max_mask] *= _max[_max_mask] \n",
    "\n",
    "    return arr + _min\n",
    "\n",
    "def _imshow(ax, im, x_range, y_range, *, title=None, YUV2RGB=False, display_values=5, ndecimals=4):\n",
    "    assert im.shape[-1] >= 2\n",
    "    im = getattr(im, \"array\", im)\n",
    "    \n",
    "    _im = np.zeros(im.shape[:-1] + (3,), dtype=im.dtype)\n",
    "    _im[..., 1:] = im[..., :2].copy()\n",
    "    \n",
    "    _min, _max = _norm(_im)\n",
    "    \n",
    "    if YUV2RGB:\n",
    "        _im = cv2.cvtColor(_im, cv2.COLOR_YUV2RGB)\n",
    "\n",
    "    _norm(_im)\n",
    "    _im[..., 0] = 0\n",
    "\n",
    "    ax.imshow(_im.round(ndecimals), extent=x_range + y_range, origin=\"lower\")\n",
    "    \n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    \n",
    "    \n",
    "    if display_values < 1:\n",
    "        return\n",
    "    h, w, _ = _im.shape\n",
    "    xs = np.linspace(*x_range, display_values)\n",
    "    ys = np.linspace(*y_range, display_values)\n",
    "    _js = np.linspace(0, w-1, display_values).astype(int)\n",
    "    _is = np.linspace(0, h-1, display_values).astype(int)\n",
    "\n",
    "    fmt = \"\\n\".join([\"{x} | {y}\", \"R={0: .2f}\", \"G={1: .2f}\", \"B={2: .2f}\"])\n",
    "    for y, i in zip(ys, _is):\n",
    "        for x, j in zip(xs, _js):\n",
    "\n",
    "            text = fmt.format(*_denorm(_im[i, j], _min, _max), x=x, y=y)\n",
    "            if i == _is.min():\n",
    "                verticalalignment = \"bottom\"\n",
    "            elif i == _is.max():\n",
    "                verticalalignment = \"top\"\n",
    "            else:\n",
    "                verticalalignment = \"center\"\n",
    "\n",
    "            if j == _js.min():\n",
    "                horizontalalignment = \"left\"\n",
    "            elif j == _js.max():\n",
    "                horizontalalignment = \"right\"\n",
    "            else:\n",
    "                horizontalalignment = \"center\"\n",
    "\n",
    "            ax.text(x,y, s=text, \n",
    "                    bbox=dict(facecolor=\"white\", alpha=0.5),\n",
    "                    horizontalalignment=horizontalalignment,\n",
    "                    verticalalignment=verticalalignment,\n",
    "                   )\n",
    "            ax.scatter(x, y, marker=\"x\", color=\"white\")\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing helper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient printing\n",
    "\n",
    "We compute the actual $\\vec{\\sigma^2}$ and $\\vec{w}$ indirectly from learned network parameters $\\vec{s}$ and $\\vec{v}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\sigma^2_i \n",
    "        &= \\epsilon + \\exp(s_i), \\quad s \\in \\mathbb{R} \\\\\n",
    "    w_i \n",
    "        &= \\dfrac{\\mathrm{sigmoid}(v_i)}{\\sum_{j} \\mathrm{sigmoid}(v_j)}, \\quad v_i \\in \\mathbb{R} \\\\        \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the network, we can only observe the gradients w.r.t. the learned parameters:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dfrac{\\partial \\mathcal{F}_\\Theta(X)}{\\partial \\vec{s}}\n",
    "    &= \\dfrac{\\partial \\mathcal{F}_\\Theta(X)}{\\partial \\vec{\\sigma^2}} \\cdot \n",
    "    \\dfrac{\\partial \\vec{\\sigma^2}}{\\partial \\vec{s}}\\\\\n",
    "\\dfrac{\\partial \\mathcal{F}_\\Theta(X)}{\\partial \\vec{v}}\n",
    "    &= \\dfrac{\\partial \\mathcal{F}_\\Theta(X)}{\\partial \\vec{w}} \\cdot \n",
    "    \\dfrac{\\partial \\vec{w}}{\\partial \\vec{v}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence, the actual gradients of the encoding w.r.t. the GMM paramters are \n",
    "(some indexing is missing, but it should be straighforward to complement those):\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\dfrac{\\partial \\mathcal{F}_\\Theta(X)}{\\partial \\vec{\\sigma^2}}\n",
    "    &= \\dfrac{\\partial \\mathcal{F}_\\Theta(X)}{\\partial \\vec{s}} \\cdot\n",
    "        \\left(\\dfrac{\\partial \\vec{\\sigma^2}}{\\partial \\vec{s}}\\right)^{-1} \\\\\n",
    "    &= \\dfrac{\\partial \\mathcal{F}_\\Theta(X)}{\\partial \\vec{s}} \\cdot\n",
    "         \\dfrac{1}{\\exp(\\vec{s})} \\\\\n",
    "\\dfrac{\\partial \\mathcal{F}_\\Theta(X)}{\\partial \\vec{w}}\n",
    "    &= \\dfrac{\\partial \\mathcal{F}_\\Theta(X)}{\\partial \\vec{v}}\n",
    "        \\left(\\dfrac{\\partial \\vec{w}}{\\partial \\vec{v}}\\right)^{-1} \\\\\n",
    "    &= \\dfrac{\\partial \\mathcal{F}_\\Theta(X)}{\\partial \\vec{v}} \\circ\n",
    "    \\left(\\vec{w}\\circ\\vec{1}^T \\circ \\left(\\left(\\mathbf{1} - \\vec{1}\\circ\\vec{w}^T\\right) \\cdot \\left(\\vec{1}-\\mathrm{sigmoid}(\\vec{v}^T)\\right)\\right)\\right)^{-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Following code cofirms the computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== _sig Grad: ==========\n",
      "Direct:\n",
      " [[ 5.46159784  1.49607285  2.43134807]\n",
      " [ 1.55475592  5.87558033 24.14606337]]\n",
      "Indirect:\n",
      " [[ 5.46159784  1.49607285  2.43134807]\n",
      " [ 1.55475592  5.87558033 24.14606337]]\n",
      "========== sig Grad: ==========\n",
      "Direct:\n",
      " [[2.71828183 2.71828183 2.71828183]\n",
      " [2.71828183 2.71828183 2.71828183]]\n",
      "Indirect:\n",
      " [[2.71828183 2.71828183 2.71828183]\n",
      " [2.71828183 2.71828183 2.71828183]]\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3)\n",
    "data = np.random.randn(*shape)\n",
    "_sig = chainer.Variable(data, requires_grad=True)\n",
    "eps = 1e-2\n",
    "sig = eps + F.exp(_sig)\n",
    "\n",
    "sig.grad = np.full(sig.shape, fill_value=np.e, dtype=sig.dtype)\n",
    "_sig.grad = None\n",
    "\n",
    "with chainer.force_backprop_mode():\n",
    "    sig.backward(retain_grad=True)\n",
    "\n",
    "print(\"==\" * 5, \"_sig Grad:\", \"==\" * 5)\n",
    "print(\"Direct:\\n\", _sig.grad)\n",
    "print(\"Indirect:\\n\",  (sig.array - eps) * sig.grad)\n",
    "\n",
    "print(\"==\" * 5, \"sig Grad:\", \"==\" * 5)\n",
    "print(\"Direct:\\n\", sig.grad)\n",
    "print(\"Indirect:\\n\", _sig.grad / (sig.array - eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== _w Grad: ==========\n",
      "Direct:\n",
      " [-0.12147768  0.07729177  0.04418591]\n",
      "Grad sum:\n",
      " 0.0 True\n",
      "Indirect:\n",
      " [-0.12147767  0.07729178  0.0441859 ]\n",
      "is close? \n",
      " [ True  True  True]\n",
      "========== w Grad: ==========\n",
      "Direct:\n",
      " [-0.17349128  0.54639417  0.25595254]\n",
      "Indirect2:\n",
      " [-0.32712893  0.39275649  0.1023149 ]\n",
      "is close?:\n",
      " [False False False]\n"
     ]
    }
   ],
   "source": [
    "shape = (3,)\n",
    "rnd = np.random.RandomState(42)\n",
    "\n",
    "data = rnd.randn(*shape).astype(np.float32)\n",
    "# data = np.ones(shape)\n",
    "# data = np.arange(shape[0], dtype=float)\n",
    "_w = chainer.Variable(data, requires_grad=True)\n",
    "\n",
    "def f(x):\n",
    "    return F.sigmoid(x) / F.sum(F.sigmoid(x))\n",
    "\n",
    "def f2(x):\n",
    "    return F.exp(x) / F.sum(F.exp(x))\n",
    "\n",
    "def grad(x, y):\n",
    "    \"\"\" Source: http://saitcelebi.com/tut/output/part2.html#back_propagation_phase\"\"\"\n",
    "\n",
    "    y2 = (1 - F.sigmoid(y).array).reshape(-1, 1)\n",
    "    return grad2(x, y) * y2\n",
    "\n",
    "def grad2(x, *_):\n",
    "    \"\"\" Source: http://saitcelebi.com/tut/output/part2.html#back_propagation_phase\"\"\"\n",
    "    \n",
    "    assert x.ndim == 1\n",
    "    n, = x.shape\n",
    "    \n",
    "    y0 = np.matmul(x.reshape(-1, 1), np.ones((1, n)))\n",
    "    y1 = np.identity(n) - np.matmul(np.ones((n, 1)), x.reshape(1, -1))\n",
    "    return y0 * y1\n",
    "\n",
    "def encode(w):\n",
    "    global rnd\n",
    "    #return F.sum(w)\n",
    "    _data0 = rnd.randn(*w.shape).astype(w.dtype)\n",
    "    #print(\"_data0:\\n\", _data0)\n",
    "    # simulates the soft-assignment\n",
    "    y = w * _data0\n",
    "    y /= F.sum(y)\n",
    "    \n",
    "    # simulates the encoding\n",
    "    _data1 = rnd.rand(*w.shape).astype(w.dtype)\n",
    "    #print(\"_data1:\\n\", _data1)\n",
    "    y = y.array * _data1 / F.sqrt(w)\n",
    "    return F.sum(y)\n",
    "\n",
    "\n",
    "if 0:\n",
    "    w = f(_w)\n",
    "    grad_matrix = grad(w.array, _w.array)\n",
    "else:\n",
    "    w = f2(_w)\n",
    "    grad_matrix = grad2(w.array, _w.array)\n",
    "\n",
    "inv_grad_matrix = np.linalg.pinv(grad_matrix)\n",
    "if not np.allclose(inv_grad_matrix @ grad_matrix, np.eye(shape[0])):\n",
    "    print(grad_matrix)\n",
    "    print(inv_grad_matrix)\n",
    "    print((inv_grad_matrix @ grad_matrix).round(6))\n",
    "    \n",
    "_w.grad = None\n",
    "\n",
    "with chainer.force_backprop_mode():\n",
    "    encode(w).backward(retain_grad=True)\n",
    "\n",
    "\n",
    "# the grads must sum up to 0!\n",
    "print(\"==\" * 5, \"_w Grad:\", \"==\" * 5)\n",
    "print(\"Direct:\\n\", _w.grad)\n",
    "print(\"Grad sum:\\n\", _w.grad.sum(), np.isclose(_w.grad.sum(), 0))\n",
    "\n",
    "ind_grad = grad_matrix @ w.grad\n",
    "print(\"Indirect:\\n\", ind_grad)\n",
    "print(\"is close? \\n\", np.isclose(_w.grad, ind_grad))\n",
    "\n",
    "print(\"==\" * 5, \"w Grad:\", \"==\" * 5)\n",
    "print(\"Direct:\\n\", w.grad)\n",
    "# ind_grad = np.linalg.solve(grad_matrix, _w.grad)\n",
    "ind_grad = inv_grad_matrix @ _w.grad\n",
    "\n",
    "print(\"Indirect2:\\n\", ind_grad)\n",
    "print(\"is close?:\\n\", np.isclose(ind_grad, w.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print(params, data_name=\"array\", headers=[\"\\u03BC\", \"\\u03C3\", \"w\"]):\n",
    "    \n",
    "    rows = []\n",
    "    _get = lambda param: getattr(param, data_name, param)\n",
    "    p_arrays = [_get(p) for p in params]\n",
    "    for i, params in enumerate(zip(*p_arrays), 1):\n",
    "        rows.append([f\"Comp #{i}\"] + [p.round(6) for p in params])\n",
    "    print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))\n",
    "\n",
    "def print_arrays(layer):\n",
    "    _print([layer.mu.T, layer.sig.T, layer.w], \"array\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_grad(layer):\n",
    "    return layer._sig.grad / (layer.sig.array - layer.eps)\n",
    "\n",
    "def w_grad(layer):\n",
    "    \n",
    "    \"\"\" Source: http://saitcelebi.com/tut/output/part2.html#back_propagation_phase\"\"\"\n",
    "    w, _w = layer.w.array, layer._w.array\n",
    "    assert w.ndim == 1\n",
    "    n, = w.shape\n",
    "    \n",
    "    y0 = np.matmul(w.reshape(-1, 1), np.ones((1, n)))\n",
    "    y1 = np.identity(n) - np.matmul(np.ones((n, 1)), w.reshape(1, -1))\n",
    "    y2 = (1 - F.sigmoid(_w).array).reshape(-1, 1)\n",
    "    grad_matrix = y0 * y1 * y2\n",
    "    inv_grad_matrix = np.linalg.pinv(grad_matrix)\n",
    "    \n",
    "    return inv_grad_matrix @ layer._w.grad\n",
    "\n",
    "def get_w_grad(layer):\n",
    "    for p in layer._ws:\n",
    "        if p.grad is not None:\n",
    "            return p.grad\n",
    "\n",
    "def print_grad(layer, enc_part):\n",
    "    params = [\n",
    "        layer.mu.grad.T, \n",
    "        sig_grad(layer).T,\n",
    "        layer._w, \n",
    "        # w_grad(layer), \n",
    "        get_w_grad(layer),\n",
    "    ]\n",
    "    param_names = [\n",
    "        \"\\u03BC\", \n",
    "        \"\\u03C3\", \n",
    "        \"_w\", \n",
    "        # \"w*\", \n",
    "        \"w\",\n",
    "    ]\n",
    "    _print(params, \"grad\", \n",
    "           headers=[f\"\\u2202 F{enc_part} / \\u2202 {name}\" for name in param_names])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Plotter class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Plotter(abc.ABC):\n",
    "    __name__ = \"Plotter\"\n",
    "    \n",
    "    def __init__(self, max_comps=10, **kwargs):\n",
    "        self.args = Args(**kwargs)\n",
    "        print(self.args)\n",
    "        self._X = None\n",
    "        self.max_comps = max_comps\n",
    "    \n",
    "    @property\n",
    "    def X(self):\n",
    "        if self._X is None:\n",
    "            self._X = new_data_grid(self.args)\n",
    "        return self._X\n",
    "        \n",
    "    \n",
    "    def __call__(self=None, \n",
    "                 mu=Args.init_mu, \n",
    "                 sig=Args.init_sig, \n",
    "                 N=Args.n_components):\n",
    "        layer = new_layer(self.args, mu, sig, N)\n",
    "        \n",
    "        print_arrays(layer)\n",
    "        self.plot(layer, self.X)\n",
    "        \n",
    "        #plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    def encode(self, layer, X):\n",
    "        h, w, c = X.shape\n",
    "        X = chainer.Variable(X)\n",
    "        \n",
    "        enc = layer(X.reshape(h*w, 1, c))\n",
    "        enc = enc.reshape(h*w, 2, layer.n_components, layer.in_size)\n",
    "        \n",
    "        if layer.n_components > self.max_comps:\n",
    "            print(\"More than {0} components! Plotting only the first {0}!\".format(self.max_comps), \n",
    "                  file=sys.stderr)\n",
    "            enc = enc[:, :, :self.max_comps]\n",
    "        \n",
    "        mu_enc, sig_enc = enc[:, 0], enc[:, 1]\n",
    "        \n",
    "        return X, mu_enc, sig_enc\n",
    "        \n",
    "    \n",
    "    def interact(self, height=None,\n",
    "                 mu=(-10, 10, 0.1), \n",
    "                 sig=(0.1, 100, 0.1),\n",
    "                 N=(1, 32, 1)):\n",
    "        _plot = interactive(self, mu=mu, sig=sig, N=N)\n",
    "        \n",
    "        # set fixed size if flickering is annoying\n",
    "        if height is not None:\n",
    "            output = _plot.children[-1]\n",
    "            output.layout.height = f\"{height}px\"\n",
    "        \n",
    "        return _plot\n",
    "        \n",
    "    def plot(self, layer, X):\n",
    "        fig, axs = plt.subplots(1,2, figsize=(16,8))\n",
    "        fig.suptitle(\"Data and the GMM\")\n",
    "        \n",
    "        layer.plot(ax=axs[1])\n",
    "        axs[1].set_xlim(*self.args.x_range)\n",
    "        axs[1].set_ylim(*self.args.y_range)\n",
    "        ranges = self.args.x_range, self.args.y_range\n",
    "        _imshow(axs[0], X, *ranges, title=\"Data\")\n",
    "        \n",
    "        self.plot_core(layer, X)\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def plot_core(self, layer, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingPlotter(Plotter):\n",
    "    \n",
    "    def plot_core(self, layer, X):\n",
    "        n0, n1, c = X.shape\n",
    "        X, mu_enc, sig_enc = self.encode(layer, X)\n",
    "            \n",
    "        n_comp = mu_enc.shape[1]\n",
    "        \n",
    "        \n",
    "        for i in np.arange(n_comp):\n",
    "            \n",
    "            fig, axs = plt.subplots(1, 2, figsize=(16,8))\n",
    "            fig.suptitle(f\"Component #{i}\")\n",
    "            fmt = \"$\\mathcal{{F}}_{{{param}_{{\" + str(i) + \"}}}}(x)$\"\n",
    "            for j, (_x, param) in enumerate(zip([mu_enc, sig_enc], [\"\\mu\", \"\\sigma^2\"])):\n",
    "                title = fmt.format(param=param)\n",
    "                _im = _x[:, i].reshape(n0, n1, 2)\n",
    "                ranges = self.args.x_range, self.args.y_range\n",
    "                _imshow(axs[j], _im, *ranges, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args(n_components=3, n_dims=2, init_mu=5, rnd_mu=True, init_sig=1, eps=0, x_range=(-10, 10), y_range=(-10, 10), n_samples=25, seed=42)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c369df1dbf3b434d8a4c310f652c811e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=5.0, description='mu', max=10.0, min=-10.0), FloatSlider(value=1.0, de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter = EncodingPlotter(\n",
    "    eps=0,\n",
    ")\n",
    "\n",
    "plotter.interact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradPlotter(Plotter):\n",
    "    \n",
    "    def plot_core(self, layer, X):\n",
    "       \n",
    "        n0,n1,c = X.shape\n",
    "        n0, n1, c = X.shape\n",
    "        X, mu_enc, sig_enc = self.encode(layer, X)\n",
    "        \n",
    "        \n",
    "        _title = f\"All components\"\n",
    "        print(f\"=== {_title} ===\")\n",
    "        title_fmt = \"$\\dfrac{{\\partial}}{{\\partial x}}\\mathcal{{F}}_{{{param}}}(x)$\" \n",
    "        \n",
    "        enc_names = [\"\\mu\", \"\\sigma^2\"]#, \"\\Theta\"]\n",
    "        encs = [mu_enc, sig_enc]#, F.concat([mu_enc, sig_enc], axis=1)]\n",
    "        \n",
    "        fig, axs = self.plot_grad(layer, X, encs, enc_names, \n",
    "                                  no_grad_print=False,\n",
    "                                  title_fmt=title_fmt)\n",
    "        fig.suptitle(_title)\n",
    "        \n",
    "        \n",
    "        for i in np.arange(mu_enc.shape[1]):\n",
    "            \n",
    "            title_fmt = \"$\\dfrac{{\\partial}}{{\\partial x}}\\mathcal{{F}}_{{{param}_{{\" + str(i) + \"}}}}(x)$\" \n",
    "\n",
    "            enc_names = [\"\\mu\", \"\\sigma^2\"]#, \"\\Theta\"]\n",
    "            encs = [mu_enc[:, i], sig_enc[:, i]]\n",
    "            # encs.append(F.concat(encs, axis=-1))\n",
    "            \n",
    "            fig, axs = self.plot_grad(layer, X, encs, enc_names, \n",
    "                                      no_grad_print=True,\n",
    "                                      title_fmt=title_fmt)\n",
    "\n",
    "            _title = f\"Component #{i}\"\n",
    "            fig.suptitle(_title)\n",
    "        \n",
    "    def plot_grad(self, layer, X, encs, enc_names, *, no_grad_print, title_fmt):\n",
    "        \n",
    "        fig, axs = plt.subplots(1, len(encs), figsize=(8*len(encs), 8))\n",
    "        \n",
    "        for ax, _enc, enc_name in zip(axs, encs, enc_names):\n",
    "            X.grad = None\n",
    "            layer.cleargrads()\n",
    "            F.sum(_enc).backward(retain_grad=True)\n",
    "            title = title_fmt.format(param=enc_name)\n",
    "            if \"mu\" in enc_name:\n",
    "                enc_part = \"\\u03BC\"\n",
    "                \n",
    "            elif \"sig\" in enc_name:\n",
    "                enc_part = \"\\u03C3\"\n",
    "                \n",
    "            else:\n",
    "                enc_part = \"\\u03B8\"\n",
    "            \n",
    "            ranges = self.args.x_range, self.args.y_range\n",
    "            _imshow(ax, X.grad, *ranges, title=title)    \n",
    "            \n",
    "            \n",
    "            if no_grad_print:\n",
    "                continue\n",
    "            \n",
    "            print_grad(layer, \n",
    "                enc_part=\"\\u03BC\" if \"mu\" in enc_name else \"\\u03C3\" if \"sig\" in enc_name else \"\\u03B8\" )\n",
    "            \n",
    "        return fig, axs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args(n_components=3, n_dims=2, init_mu=5, rnd_mu=True, init_sig=1, eps=0, x_range=(-10, 10), y_range=(-10, 10), n_samples=25, seed=42)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4199a214db4c378699e3954b9a2755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=5.0, description='mu', max=10.0, min=-10.0), FloatSlider(value=1.0, de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plotter = GradPlotter(\n",
    "    eps=0,\n",
    "    rnd_mu=True,\n",
    ")\n",
    "\n",
    "plotter.interact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = new_layer(plotter.args)\n",
    "\n",
    "layer._sig, layer.sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
