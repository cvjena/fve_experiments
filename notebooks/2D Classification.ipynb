{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pyaml\n",
    "\n",
    "import cupy\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "\n",
    "from chainer_addons.training.extensions.progress_bar import JupyterProgressBar\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils._testing import  ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "\n",
    "from fve_layer.backends.chainer.links import FVELayer\n",
    "from fve_layer.backends.chainer.links import FVELayer_noEM\n",
    "from fve_layer.common.visualization import draw_ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    def __str__(self):\n",
    "        return pyaml.dump(dict(Arguments=self.__dict__), sort_dicts=False)\n",
    "\n",
    "    n_dims:                int = 2\n",
    "    n_samples:             int = 128\n",
    "    n_classes:             int = 4\n",
    "    sample_std:            float = 1\n",
    "        \n",
    "    data_scale:            float = 10\n",
    "    data_shift:            np.ndarray = np.array([5,5])\n",
    "    \n",
    "    n_components:          int = 1\n",
    "    \n",
    "    fve_linear:            bool = False\n",
    "    fve_only_mu:           bool = True\n",
    "    fve_only_sig:          bool = False\n",
    "    fve_normalize:         bool = True\n",
    "    fve_no_update:         bool = False\n",
    "        \n",
    "    batch_size:            int = 32\n",
    "    learning_rate:         float = 1e-2\n",
    "    seed:                  float = 42\n",
    "    device:                int = -1\n",
    "    \n",
    "args = Args()\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
      "source": [
    "def train(data: Data, clf: FVEClassifier, \n",
    "          batch_size: int, \n",
    "          learning_rate: float,\n",
    "          triggers: dict, \n",
    "          device: int = -1,\n",
    "          eval_data: Data = None):\n",
    "    \n",
    "    it = chainer.iterators.SerialIterator(data, batch_size=min(batch_size, len(data)))\n",
    "\n",
    "#     optimizer = chainer.optimizers.MomentumSGD(lr=learning_rate)\n",
    "    optimizer:  chainer.optimizers.Adam = chainer.optimizers.Adam(alpha=learning_rate)\n",
    "    optimizer.setup(clf)\n",
    "    optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(5e-4))\n",
    "    \n",
    "    updater = chainer.training.updaters.StandardUpdater(it, optimizer, \n",
    "                                                        converter=concat_var_examples,\n",
    "                                                        device=device)\n",
    "    \n",
    "    \n",
    "    trainer = chainer.training.Trainer(updater=updater, stop_trigger=triggers[\"stop\"], out=\"/tmp/chainer_logs\")\n",
    "\n",
    "    print_values= [\"epoch\", \"main/accu\", \"main/loss\", \"main/dist\"]\n",
    "    \n",
    "    if eval_data is not None:\n",
    "        eval_it = chainer.iterators.SerialIterator(eval_data, batch_size=batch_size, \n",
    "                                                   repeat=False, shuffle=False)\n",
    "        evaluator = chainer.training.extensions.Evaluator(eval_it, target=clf, device=updater.device)\n",
    "        eval_name = \"val\"\n",
    "        evaluator.default_name = eval_name\n",
    "        trainer.extend(evaluator, trigger=triggers[\"log\"])\n",
    "        print_values.extend([f\"{eval_name}/main/accu\", f\"{eval_name}/main/loss\", f\"{eval_name}/main/dist\"])\n",
    "\n",
    "    \n",
    "    trainer.extend(chainer.training.extensions.LogReport(trigger=triggers[\"log\"]))\n",
    "    trainer.extend(chainer.training.extensions.PrintReport(print_values), trigger=triggers[\"log\"])\n",
    "    trainer.extend(JupyterProgressBar(update_interval=triggers[\"progress_bar\"]))\n",
    "    \n",
    "    trainer.extend(zero_grads(triggers[\"stop\"]))\n",
    "\n",
    "    trainer.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_var_examples(batch, device=None):\n",
    "    \n",
    "    Xs, ys = zip(*batch)\n",
    "    \n",
    "    Xs = F.stack(Xs, axis=0)\n",
    "    ys = np.hstack(ys)\n",
    "    return Xs, ys\n",
    "\n",
    "def _get_array(var):\n",
    "    return getattr(var, \"array\", var)\n",
    "\n",
    "def _new_data(n_samples, n_dims, mean, std, rnd=None):\n",
    "    \"\"\" generates normal distributed data \"\"\"\n",
    "    rnd = rnd or np.random.RandomState()\n",
    "    return rnd.normal(loc=mean, scale=std, size=(n_samples, n_dims))\n",
    "\n",
    "def _new_data2(n_samples, n_dims, mean, std, rnd=None, *, n_circles=4):\n",
    "    \"\"\" generates data as regular circles \"\"\"\n",
    "    assert n_dims == 2, \"Only 2-D data is supported\"\n",
    "    \n",
    "    factors = np.linspace(0.5, 2, num=n_circles) # eg. default is [0.5, 1.0, 1.5, 2.0]\n",
    "    # in each row is one of the factors multiplied with the std    \n",
    "    _stds = np.outer(factors, std) \n",
    "    _n_samples = n_samples // n_circles\n",
    "    \n",
    "    _X = np.zeros((n_samples, n_dims), dtype=mean.dtype)\n",
    "    \n",
    "    for i, _std in enumerate(_stds):\n",
    "        # distribute the samples in a circle\n",
    "        angles = np.linspace(0, 2*np.pi, _n_samples, endpoint=False)\n",
    "        xs = np.vstack([np.cos(angles), np.sin(angles)]).T\n",
    "        \n",
    "        i0 = i * _n_samples\n",
    "        i1 = i0 + _n_samples\n",
    "        _X[i0:i1] = xs * _std + mean\n",
    "    \n",
    "    return _X\n",
    "\n",
    "def _new_rnd(seed, is_evaluation):\n",
    "    if seed is not None and is_evaluation:\n",
    "        seed = 2**31 - seed\n",
    "        \n",
    "    return np.random.RandomState(seed)\n",
    "\n",
    "class Data(chainer.dataset.DatasetMixin):\n",
    "    @classmethod\n",
    "    def new(cls, args: Args, evaluation: bool=False):\n",
    "        return cls(\n",
    "            n_classes=args.n_classes,\n",
    "            n_dims=args.n_dims,\n",
    "            n_samples=args.n_samples,\n",
    "            std=args.sample_std,\n",
    "            scale=args.data_scale,\n",
    "            shift=args.data_shift,\n",
    "            rnd=_new_rnd(args.seed, evaluation)\n",
    "        )\n",
    "    \n",
    "    def __init__(self, *, n_classes, n_dims, n_samples, std=1, dtype=np.float32, scale=None, shift=None, rnd=None):\n",
    "        super(Data, self).__init__()\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "        self.X = np.zeros((n_classes * n_samples, n_dims), dtype=dtype)\n",
    "        self.y = np.zeros(n_classes * n_samples, dtype=np.int32)\n",
    "        \n",
    "        # distribute the class centers evenly in a circle (in the 1st two dimensions)\n",
    "        _comp_pos = np.linspace(0, 2*np.pi, n_classes, endpoint=False)\n",
    "        self._means = np.zeros((n_classes, n_dims), dtype=np.float32)\n",
    "        self._means[:, :2] = np.vstack([np.cos(_comp_pos), np.sin(_comp_pos)]).T\n",
    "        \n",
    "        self._std = np.ones(n_dims, dtype=np.float32) / n_classes * std\n",
    "        \n",
    "\n",
    "        for i, mean in enumerate(self._means):\n",
    "            _X = _new_data(n_samples, n_dims, mean, self._std, rnd)\n",
    "#             _X = _new_data2(n_samples, mean, self._std, rnd)\n",
    "            n0 = i * self.n_samples\n",
    "            n1 = n0 + self.n_samples\n",
    "            self.X[n0: n1] = _X\n",
    "            self.y[n0: n1] = i\n",
    "        \n",
    "        if scale is not None:\n",
    "            self.X *= scale\n",
    "            self._means *= scale\n",
    "            self._std *= scale\n",
    "            \n",
    "        if shift is not None:\n",
    "            self.X += shift\n",
    "            self._means += shift\n",
    "        \n",
    "        self.X = chainer.Variable(self.X, name=\"data\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "    def plot(self, \n",
    "             ax: plt.Axes = None, \n",
    "             cm: colors.ListedColormap = None,\n",
    "             marker: str = None,\n",
    "             alpha: float = 1.0,\n",
    "             plot_grad=False,\n",
    "             **kwargs,\n",
    "            ):\n",
    "        \n",
    "        ax = ax or plt.gca()\n",
    "        cm = cm or plt.cm.viridis\n",
    "        _x = chainer.cuda.to_cpu(_get_array(self.X))\n",
    "        _y = chainer.cuda.to_cpu(self.y)\n",
    "        ax.scatter(*_x.T, c=cm(_y / self.n_classes), marker=marker, alpha=alpha)\n",
    "        if plot_grad and self.X.grad is not None:\n",
    "            self._plot_grads(ax, cm, **kwargs)\n",
    "        return ax\n",
    "    \n",
    "    \n",
    "    def _plot_grads(self, \n",
    "                    ax: plt.Axes, \n",
    "                    cm: colors.ListedColormap, \n",
    "                    norm: bool = True, \n",
    "                    xp=np):\n",
    "        \n",
    "        x0 = self.X.array\n",
    "        step_size = 1e-1\n",
    "        \n",
    "        grad = self.X.grad.copy()\n",
    "        if norm:\n",
    "            grad_norm = xp.sqrt(xp.sum(grad **2, axis=1))\n",
    "            mask = grad_norm != 0\n",
    "            grad[mask] = grad[mask] / grad_norm[mask, None]\n",
    "        \n",
    "        for (x, y), (dx, dy), lab in zip(x0, -grad, self.y):\n",
    "            if dx == dy == 0: \n",
    "                continue\n",
    "            ax.arrow(x,y, dx, dy, width=2e-1, facecolor=cm(lab / self.n_classes))\n",
    "        \n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(chainer.Chain):\n",
    "    \n",
    "    @classmethod\n",
    "    def new(cls, opts: Args, *a, **kw):\n",
    "        args, kwargs = cls.extract_args(opts, *a, **kw)\n",
    "        return cls(*args, **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def extract_args(cls, opts: Args, *a, **kw):\n",
    "        args = tuple(a)\n",
    "        kwargs = dict(kw,\n",
    "            n_classes=opts.n_classes,\n",
    "            in_size=opts.n_dims,\n",
    "        )\n",
    "        return args, kwargs\n",
    "\n",
    "    def report(self, **values):\n",
    "        chainer.report(values, self)        \n",
    "        \n",
    "    def __init__(self, n_classes, in_size=2):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        with self.init_scope():\n",
    "            self.fc = L.Linear(in_size=in_size, out_size=n_classes)\n",
    "        \n",
    "    def decision_function(self, X):\n",
    "        features = self.encode(X)\n",
    "        return self.classify(features)\n",
    "        \n",
    "    def encode(self, X):\n",
    "        return X\n",
    "    \n",
    "    def classify(self, features):\n",
    "        #return self.fc(F.dropout(features))\n",
    "        return self.fc(features)\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "        logits = self.decision_function(X)\n",
    "        \n",
    "        accu, loss = F.accuracy(logits, y), F.softmax_cross_entropy(logits, y)\n",
    "        \n",
    "        self.report(accu=accu, loss=loss)\n",
    "        \n",
    "        # uncomment this to plot gradients of the FVE w.r.t. the inputs only\n",
    "        #return F.sum(features)\n",
    "        \n",
    "        # returning the classification loss, plots the gradients of the classification\n",
    "        # w.r.t. the inputs (composed of the FVE gradient and classifier gradient)\n",
    "        return loss\n",
    "    \n",
    "    def plot(self, ax: plt.Axes = None, cm: colors.ListedColormap = None):\n",
    "        ax = ax or plt.gca()\n",
    "        cm = cm or plt.cm.viridis\n",
    "                \n",
    "        xlim, ylim = ax.get_xlim(), ax.get_ylim()\n",
    "        xs = np.array(xlim)\n",
    "        \n",
    "        tri = np.tri(self.n_classes, self.n_classes, -1, dtype=np.bool)\n",
    "        i0s, i1s = np.where(tri)\n",
    "        \n",
    "        W, bias = self.fc.W.array, self.fc.b.array\n",
    "        for i0, i1 in zip(i0s, i1s):\n",
    "            w0x, w0y = W[i0]\n",
    "            b0 = bias[i0]\n",
    "            \n",
    "            w1x, w1y = W[i1]\n",
    "            b1 = bias[i1]\n",
    "\n",
    "            a, b  = -(w0x-w1x) / (w0y-w1y), -(b0-b1) / (w0y-w1y)\n",
    "\n",
    "            ys = a * xs + b\n",
    "\n",
    "            ax.plot(xs, ys, c=\"k\")\n",
    "                \n",
    "            \n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "        \n",
    "        return ax\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FVEClassifier(Classifier):\n",
    "    \n",
    "    @classmethod\n",
    "    def extract_args(cls, opts: Args, *a, **kw):\n",
    "        args, kwargs = super(FVEClassifier, cls).extract_args(opts, *a, **kw)\n",
    "        \n",
    "        n_comps = opts.n_components if opts.n_components > 0 else opts.n_classes\n",
    "        kwargs = dict(kwargs,\n",
    "                      in_size=opts.n_dims,\n",
    "                      n_components=n_comps,\n",
    "                      normalize=opts.fve_normalize,\n",
    "                      only_mu=opts.fve_only_mu,\n",
    "                      only_sig=opts.fve_only_sig,\n",
    "                      linear_clf=opts.fve_linear,\n",
    "                      no_update=opts.fve_no_update,\n",
    "                     )\n",
    "        \n",
    "        return args, kwargs\n",
    "    def __init__(self, n_classes, in_size=2, *, \n",
    "                 n_components=1, fve_class=FVELayer, \n",
    "                 linear_clf=False,\n",
    "                 normalize=False,\n",
    "                 only_mu=False,\n",
    "                 only_sig=False,\n",
    "                 no_update=False,\n",
    "                 **kwargs):\n",
    "        \n",
    "        factor = 1 if (only_mu or only_sig) else 2\n",
    "        encoding_size = factor * in_size * n_components\n",
    "\n",
    "        super(FVEClassifier, self).__init__(n_classes, in_size=encoding_size)\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.fve_layer = fve_class(in_size=in_size, n_components=n_components, **kwargs)\n",
    "            if linear_clf:\n",
    "                post_fve = None\n",
    "            else:\n",
    "                post_fve = L.Linear(in_size=encoding_size, out_size=encoding_size)\n",
    "            self.post_fve = post_fve\n",
    "            \n",
    "            self.add_persistent(\"is_linear\", linear_clf)\n",
    "            self.add_persistent(\"normalize\", normalize)\n",
    "            self.add_persistent(\"only_mu\", only_mu)\n",
    "            self.add_persistent(\"only_sig\", only_sig)\n",
    "            self.add_persistent(\"no_update\", no_update)\n",
    "\n",
    "        \n",
    "    def encode(self, X):\n",
    "        x = F.expand_dims(X, axis=1)\n",
    "        \n",
    "        if self.no_update:\n",
    "            with chainer.using_config(\"train\", False), chainer.no_backprop_mode():\n",
    "                enc = self.fve_layer(x)\n",
    "\n",
    "        else:\n",
    "            enc = self.fve_layer(x)\n",
    "\n",
    "        dist = self.fve_layer.mahalanobis_dist(x)\n",
    "        assingment = self.fve_layer.soft_assignment(x)\n",
    "        mean_dist = F.mean(F.sum(assingment * dist, axis=-1))\n",
    "        self.report(dist=mean_dist)\n",
    "        \n",
    "        \n",
    "        if self.normalize:\n",
    "            enc = F.normalize(enc)\n",
    "                \n",
    "        _, size = enc.shape\n",
    "\n",
    "        if self.only_mu and self.only_sig:\n",
    "            raise ValueError(\"Setting both only_mu and only_sig is not allowed!\")\n",
    "            \n",
    "        elif self.only_mu and not self.only_sig:\n",
    "            enc = enc[:, :size//2]\n",
    "\n",
    "        elif not self.only_mu and self.only_sig:\n",
    "            enc = enc[:, size//2:]\n",
    "\n",
    "        return enc\n",
    "    \n",
    "    def classify(self, features):\n",
    "        if self.post_fve is not None:\n",
    "            features = F.relu(self.post_fve(features))\n",
    "            \n",
    "        elif not self.is_linear:\n",
    "            features = F.relu(features)\n",
    "            \n",
    "        return super(FVEClassifier, self).classify(features)  \n",
    "        \n",
    "    \n",
    "    def plot(self, ax: plt.Axes = None, cm: colors.ListedColormap = None):\n",
    "        if self.fve_layer.in_size != 2: return\n",
    "        \n",
    "        mu = _get_array(self.fve_layer.mu)\n",
    "        sig = _get_array(self.fve_layer.sig)\n",
    "        \n",
    "        for _mu, _sig in zip(mu.T, sig.T):\n",
    "            ax.scatter(*_mu, marker=\"x\", color=\"black\")\n",
    "            draw_ellipse(_mu, _sig, nsig=2, ax=ax, alpha=0.3)\n",
    "            \n",
    "        \n",
    "        return ax\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=None):\n",
    "    np.random.seed(seed)\n",
    "    cupy.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def zero_grads(stop_trigger):\n",
    "    enabled = stop_trigger[1] == \"epoch\"\n",
    "    \n",
    "    @chainer.training.extension.make_extension(trigger=(1, \"epoch\"))\n",
    "    def inner(trainer):\n",
    "        if not enabled:\n",
    "            return \n",
    "        \n",
    "        updater = trainer.updater\n",
    "        if updater.epoch == stop_trigger[0]:\n",
    "            return \n",
    "        \n",
    "        data = updater.get_iterator(\"main\").dataset\n",
    "\n",
    "        data.X.cleargrad()\n",
    "\n",
    "    return inner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_decisions(X, y, clf, ax=None, resolution=0.01, alpha=0.5):\n",
    "    if X.shape[1] != 2:\n",
    "        return \n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "\n",
    "    X, y = data.X.array, data.y\n",
    "\n",
    "    # create the grid for background colors\n",
    "    x_min, x_max = X[:, 0].min() , X[:, 0].max() \n",
    "    y_min, y_max = X[:, 1].min() , X[:, 1].max() \n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max*(1 + resolution), resolution*(x_max-x_min)), \n",
    "        np.arange(y_min, y_max*(1 + resolution), resolution*(y_max-y_min))\n",
    "    )\n",
    "\n",
    "    # plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]*[y_min, y_max].\n",
    "    _X = np.c_[xx.ravel(), yy.ravel()].astype(np.float32)\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        decision = clf.decision_function(_X)\n",
    "        Z = getattr(decision, \"array\", decision).argmax(axis=1)\n",
    "    else:\n",
    "        Z = clf.predict_proba(_X)[:, 1]\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_params(data: Data, clf: FVEClassifier, \n",
    "                 clf_dump: FVEClassifier = None, \n",
    "                 eval_data: Data = None,\n",
    "                 title: str = None, \n",
    "                 plot_grad: bool = False,\n",
    "                 plot_norm_grad: bool = False,\n",
    "                 fig_axs=None):\n",
    "    \n",
    "    \n",
    "    if fig_axs is None:\n",
    "        if clf_dump is None:\n",
    "            ax0 = None\n",
    "            fig, ax1 = plt.subplots(figsize=(12,12))\n",
    "            \n",
    "        else:\n",
    "            fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(16,9))\n",
    "            ax0.set_title(\"Before Training\")\n",
    "            ax1.set_title(\"After Training\")\n",
    "\n",
    "    else:\n",
    "        fig, axs = fig_axs\n",
    "        try:\n",
    "            ax0, ax1 = axs\n",
    "        except:\n",
    "            ax0, ax1 = None, axs\n",
    "            \n",
    "\n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "    \n",
    "    if ax0 is not None:\n",
    "        data.plot(ax=ax0)\n",
    "        if eval_data is not None:\n",
    "            eval_data.plot(ax=ax0, marker=\"x\", alpha=0.5)\n",
    "        clf_dump.plot(ax=ax0)\n",
    "\n",
    "    data.plot(ax=ax1, plot_grad=plot_grad, norm=plot_norm_grad)\n",
    "    if eval_data is not None:\n",
    "        eval_data.plot(ax=ax1, marker=\"x\", alpha=0.5)\n",
    "    clf.plot(ax=ax1)\n",
    "\n",
    "    return fig, (ax0, ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def baseline(data, eval_data=None, clf_class=LinearSVC):\n",
    "    \n",
    "    X, y = data.X.array, data.y\n",
    "    X_val, y_val = None, None\n",
    "    \n",
    "    if eval_data is not None:\n",
    "        X_val, y_val = eval_data.X, eval_data.y\n",
    "    \n",
    "    svm = clf_class()\n",
    "\n",
    "    searcher = GridSearchCV(svm,\n",
    "        n_jobs=4, param_grid=dict(\n",
    "            C=[1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "            tol=[1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "        )).fit(X, y)\n",
    "    \n",
    "    kwargs = searcher.best_params_\n",
    "    svm = clf_class(**kwargs).fit(X, y)\n",
    "    \n",
    "    print(f\"Baseline ({clf_class.__name__}) with best params ({kwargs})\")\n",
    "    accu = svm.score(X, y)\n",
    "    print(f\"Training accu:   {accu: 10.2%}\")\n",
    "    if eval_data is not None:\n",
    "        val_accu = svm.score(X_val, y_val)\n",
    "        print(f\"Validation accu: {val_accu: 10.2%}\")\n",
    "        \n",
    "    if X.shape[1] != 2: \n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    _plot_decisions(X, y, clf=svm, alpha=0.5, ax=ax)\n",
    "        \n",
    "    # plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', alpha=0.7)\n",
    "\n",
    "    if eval_data is not None:\n",
    "        ax.scatter(X_val[:, 0], X_val[:, 1], c=y_val, marker=\"x\", edgecolors='k', alpha=0.7)\n",
    "    \n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    \n",
    "    ax.grid()\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_classifier(args: Args, data: Data, clf: Classifier, \n",
    "                       triggers: dict, \n",
    "                       eval_data: Data = None,\n",
    "                       print_params: bool = False,\n",
    "                       plot_params: bool = True,\n",
    "                       plot_decisions: bool = False,\n",
    "                       title: str = None):\n",
    "    print(clf)\n",
    "    if data.X.shape[1] == 2:\n",
    "        clf_dump = clf.copy(mode=\"copy\")\n",
    "    \n",
    "    \n",
    "    device = chainer.cuda.get_device_from_id(args.device)\n",
    "    device.use()\n",
    "    clf.to_device(device)\n",
    "    \n",
    "    data.X.to_device(device)\n",
    "    data.y = chainer.cuda.to_gpu(data.y)\n",
    "    \n",
    "    train(data, clf, \n",
    "          batch_size=args.batch_size,\n",
    "          learning_rate=args.learning_rate,\n",
    "          device=args.device,\n",
    "          triggers=triggers,\n",
    "          eval_data=eval_data)\n",
    "    \n",
    "    clf.to_cpu()\n",
    "    data.X.to_cpu()\n",
    "    data.y = chainer.cuda.to_cpu(data.y)\n",
    "    \n",
    "    if data.X.shape[1] != 2: return\n",
    "            \n",
    "    if plot_params:\n",
    "        \n",
    "        fig, (ax0, ax1) = _plot_params(data, clf, \n",
    "                                       eval_data=eval_data,\n",
    "                                       clf_dump=clf_dump, \n",
    "                                       title=title)\n",
    "    \n",
    "    else:\n",
    "        ax0 = None\n",
    "        fig, ax1 = plt.subplots(figsize=(12,12))\n",
    "        data.plot(ax1)\n",
    "        if eval_data is not None:\n",
    "            eval_data.plot(ax1, marker=\"x\", alpha=0.5)\n",
    "        \n",
    "    if plot_decisions:\n",
    "        X, y = data.X.array, data.y\n",
    "        with chainer.using_config(\"train\", False):\n",
    "            #_plot_decisions(X, y, clf=clf_dump, alpha=0.3, ax=ax0)\n",
    "            _plot_decisions(X, y, clf=clf, alpha=0.3, ax=ax1)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    if print_params:\n",
    "        fmt = \"({0: 10.3f}, {1: 10.3f})\"\n",
    "\n",
    "        if args.n_components == 1:\n",
    "            _x = _get_array(data.X)\n",
    "            print(\"=== Data stats ===\")\n",
    "            print(tabulate([(fmt.format(*_x.mean(axis=0)), fmt.format(*_x.var(axis=0)))], \n",
    "                           headers=[\"\\u03BC\", \"\\u03C3\"], \n",
    "                           tablefmt=\"fancy_grid\",\n",
    "                          )\n",
    "                 )\n",
    "            print()\n",
    "\n",
    "        for i, c in enumerate([clf_dump, clf]):\n",
    "            rows = []\n",
    "\n",
    "            for _mu, _sig in zip(_get_array(c.fve_layer.mu).T, _get_array(c.fve_layer.sig).T):\n",
    "                rows.append([fmt.format(*_mu), fmt.format(*_sig)])\n",
    "\n",
    "            print(\"=== Estimated GMM Parameters ===\" if i == 1 else \"=== Initial GMM Parameters ===\")\n",
    "            print(tabulate(rows, \n",
    "                           headers=[\"\\u03BC\", \"\\u03C3\"], \n",
    "                           tablefmt=\"fancy_grid\",\n",
    "                           showindex=True,\n",
    "                          )\n",
    "                 )\n",
    "            print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradient(args: Args, data: Data, clf: Classifier, \n",
    "                     triggers: dict,\n",
    "                     eval_data: Data = None,\n",
    "                     plot_params: bool = True,\n",
    "                     plot_decisions: bool = False,\n",
    "                     title: str = None,\n",
    "                     **kwargs,\n",
    "                    ):\n",
    "    print(clf)\n",
    "    \n",
    "    train(data, clf, \n",
    "          batch_size=args.batch_size,\n",
    "          learning_rate=args.learning_rate,\n",
    "          triggers=triggers,\n",
    "          eval_data=eval_data)\n",
    "    \n",
    "    if data.X.shape[1] != 2: return\n",
    "    \n",
    "    fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(16,9))\n",
    "    \n",
    "    feat = clf.encode(data.X)\n",
    "    feat.grad = clf.xp.ones_like(feat.array)\n",
    "    feat.backward()\n",
    "    \n",
    "    ax0.set_title(\"Gradient of the encoding wrt input\")\n",
    "    _plot_params(data, clf, \n",
    "                 eval_data=eval_data,\n",
    "                 plot_grad=True, \n",
    "                 fig_axs=(fig, ax0),\n",
    "                 **kwargs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    data.X.grad = None\n",
    "    feat.grad = None\n",
    "    pred = clf.classify(feat)\n",
    "    loss = F.softmax_cross_entropy(pred, data.y)\n",
    "    loss.backward()\n",
    "    \n",
    "    ax1.set_title(\"Gradient of loss wrt input\")\n",
    "    _plot_params(data, clf, \n",
    "                 eval_data=eval_data,\n",
    "                 plot_grad=True, \n",
    "                 fig_axs=(fig, ax1),\n",
    "                 **kwargs)\n",
    "\n",
    "    if plot_decisions:\n",
    "        X, y = data.X.array, data.y\n",
    "        with chainer.using_config(\"train\", False):\n",
    "            _plot_decisions(X, y, clf=clf, alpha=0.3, ax=ax1)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small setup\n",
    "D = 2\n",
    "N = 64\n",
    "\n",
    "# medium setup\n",
    "# D = 3\n",
    "# N = 32*D\n",
    "\n",
    "# # big setup\n",
    "# D = 512\n",
    "# N = 512\n",
    "\n",
    "args = Args(\n",
    "    n_dims=D,\n",
    "    n_classes=5,\n",
    "    n_samples=N, \n",
    "    \n",
    "    sample_std=1.5, \n",
    "    data_shift=None,#np.full(D, fill_value=0, dtype=np.float32),\n",
    "    data_scale=10,\n",
    "    \n",
    "    batch_size=N*5,\n",
    "    learning_rate=1e-1,\n",
    "    \n",
    "    n_components=5,\n",
    "    \n",
    "    fve_only_mu=False,\n",
    "    fve_only_sig=False,\n",
    "    fve_normalize=False,\n",
    "    \n",
    "    fve_linear=False,\n",
    "    fve_no_update=False,\n",
    "    \n",
    "    seed=42,\n",
    "    device=0,\n",
    ")\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data.new(args)\n",
    "eval_data = Data.new(args, evaluation=True)\n",
    "if eval_data is not None:\n",
    "    eval_data.X = eval_data.X.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "baseline(data, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clfs = []\n",
    "\n",
    "iters_per_epoch = int(args.n_samples*args.n_classes / args.batch_size)\n",
    "\n",
    "default_epochs = 100\n",
    "for fve_class, epochs in [(FVELayer, None), (FVELayer_noEM, 2000)]:\n",
    "    epochs = epochs or default_epochs\n",
    "    triggers = dict(\n",
    "        stop=(epochs, \"epoch\"),\n",
    "        log=(max(1, int(epochs/5)), \"epoch\"),\n",
    "        progress_bar=iters_per_epoch*1\n",
    "    )\n",
    "\n",
    "    set_seed(args.seed)\n",
    "    kwargs = dict(\n",
    "        fve_class=fve_class,\n",
    "        init_mu=10, #data._means.T, #data.X.array.mean(axis=0)[:, None],\n",
    "        init_sig=1, #data._std[0]**2, #data.X.array.var(axis=0)[:, None],\n",
    "        \n",
    "    )\n",
    "        \n",
    "    clf = FVEClassifier.new(args, **kwargs)\n",
    "    \n",
    "    kwargs = dict(\n",
    "        args=args,\n",
    "        data=data,\n",
    "        eval_data=eval_data,\n",
    "        clf=clf,\n",
    "        triggers=triggers, \n",
    "        plot_decisions=True,\n",
    "        title=fve_class.__name__\n",
    "    )\n",
    "\n",
    "    if 1:\n",
    "        analyze_classifier(plot_params=True, **kwargs)\n",
    "    \n",
    "    else:\n",
    "        analyze_gradient(plot_norm_grad=True, **kwargs)\n",
    "\n",
    "    \n",
    "    clfs.append(clf)\n",
    "    \n",
    "#     with chainer.using_config(\"train\", False):\n",
    "#         feats = clf.encode(data.X)\n",
    "#         for cls in range(args.n_classes):\n",
    "#             print(cls)\n",
    "#             min_feat = feats.array[data.y == cls].min(axis=0).round(3)\n",
    "#             max_feat = feats.array[data.y == cls].max(axis=0).round(3)\n",
    "#             print(np.abs(np.sign(min_feat)), np.abs(np.sign(max_feat)), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mahalanobis Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _soft_assignment(data, mean, var, w):\n",
    "    \n",
    "    n_comps = mean.shape[-1]\n",
    "    _gmm = GMM(n_components=n_comps, covariance_type=\"diag\", warm_start=True)\n",
    "    \n",
    "    _gmm.covariances_ = var.T\n",
    "    _gmm.precisions_cholesky_ = 1 / np.sqrt(var.T)\n",
    "    _gmm.means_ = mean.T\n",
    "    _gmm.weights_ = w\n",
    "    \n",
    "    return _gmm.predict_proba(data)\n",
    "\n",
    "def dist(data, mean, var, w=None):\n",
    "    print(\"Input shapes:\", data.shape, mean.shape, var.shape)\n",
    "    _data = data[..., None]\n",
    "    n_comps = mean.shape[-1]\n",
    "    _mean = mean[None]\n",
    "    _var = var[None]\n",
    "    \n",
    "    if w is None:\n",
    "        w = np.ones(n_comps, dtype=_mean.dtype) / n_comps\n",
    "    \n",
    "    dist = np.sqrt( ((_data - _mean)**2 / _var).sum(axis=1) )\n",
    "    gamma = _soft_assignment(data, mean, var, w)\n",
    "    \n",
    "    \n",
    "    return (dist * gamma).sum(axis=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... to mean and variance of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mean = data.X.array.mean(axis=0)[:, None]\n",
    "_var = data.X.array.var(axis=0)[:, None]\n",
    "_data = data.X.array\n",
    "dist(_data, _mean, _var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... to $\\vec\\mu$ and $\\vec\\sigma$ used to generate the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mean = data._means.T\n",
    "_var = data._std[:, None].repeat(data.n_classes, axis=-1)**2\n",
    "_data = data.X.array\n",
    "\n",
    "dist(_data, _mean, _var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... to $\\vec\\mu$ and $\\vec\\sigma$ estimated with a GMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = data.X.array\n",
    "\n",
    "gmm = GMM(n_components=data.n_classes, covariance_type=\"diag\")\n",
    "gmm.fit(_data)\n",
    "\n",
    "_means = gmm.means_.T\n",
    "_var = gmm.covariances_.T\n",
    "_w = gmm.weights_\n",
    "\n",
    "dist(_data, _means, _var, _w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and weight distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{-np.log(1/clf.fve_layer.n_components):.5f}\")\n",
    "for clf in clfs:\n",
    "    w = _get_array(clf.fve_layer.w)\n",
    "    print(f\"{-np.sum(w * np.log(w)): .5f}\", w.round(3), w.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
